{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import ceil\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Extended pattern to remove URLs, including non-standard patterns like 'pictwittercom / Wav1bacr5j'\n",
    "    text_no_urls = re.sub(r'\\b(?:http\\S+|www\\.\\S+|[a-zA-Z0-9]+(?:[.\\/]\\S+)+)\\b', '', text)\n",
    "    \n",
    "    # Remove spaces around slashes which are not typical in URLs but used in your examples\n",
    "    text_no_extra_spaces = re.sub(r'\\s+\\/\\s+', '/', text_no_urls)\n",
    "    \n",
    "    # Remove Punctuation except for @\n",
    "    text_no_punctuation = re.sub(r'[.,:;!?\"%$()[{@\\'`\\-]', '', text_no_extra_spaces)\n",
    "    \n",
    "    # Normalize Spaces\n",
    "    text_normalized_spaces = re.sub(r'\\s+', ' ', text_no_punctuation)\n",
    "    \n",
    "    # Trim spaces\n",
    "    text_trimmed = text_normalized_spaces.strip()\n",
    "    return text_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text_spacy(line):\n",
    "    doc = nlp(line)\n",
    "    filtered_tokens = [token.text for token in doc if token.is_alpha and not token.text.startswith('@')]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janne\\AppData\\Local\\Temp\\ipykernel_16028\\3019753052.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  formatted_and_cleaned_lines = df_split.apply(lambda row: clean_text_spacy(row[0]) + \" @\" + row[1].lower(), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# sentences_50-allagree\n",
    "\n",
    "df = pd.read_csv('./data/train/Sentences_50Agree.txt', sep='\\t', encoding='ISO-8859-1', header=None, names=['blaa'])\n",
    "df_split = df['blaa'].str.split('.@', expand=True)\n",
    "df_split.columns = ['text','sentiment']\n",
    "df_split['text'] = df_split['text'].str.strip()\n",
    "df_split['sentiment'] = df_split['sentiment'].str.strip()\n",
    "\n",
    "formatted_and_cleaned_lines = df_split.apply(lambda row: clean_text_spacy(row[0]) + \" @\" + row[1].lower(), axis=1)\n",
    "\n",
    "with open('./data/train/clean_train_texts/Sentences_50Agree_spacy.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in formatted_and_cleaned_lines:\n",
    "        if len(line) > 10:  # Only write lines that are longer than the minimum length\n",
    "            file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janne\\AppData\\Local\\Temp\\ipykernel_16028\\2814373075.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  formatted_and_cleaned_lines = df_no_irrelevant_values.apply(lambda row: clean_text_spacy(row[1]) + \" @\" + row[0].lower(), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# twitter_training.csv cleaning\n",
    "\n",
    "# Load the DataFrame\n",
    "column_names = ['0', '1', '2', '3']\n",
    "df = pd.read_csv('./data/train/twitter_training.csv', header=None, names=column_names, encoding='ISO-8859-1')\n",
    "df.drop(['0', '1'], axis=1, inplace=True)\n",
    "\n",
    "# Filter out 'Irrelevant' values\n",
    "df_no_irrelevant_values = df[df['2'] != 'Irrelevant']\n",
    "df_no_irrelevant_values = df_no_irrelevant_values.dropna(subset=['2', '3'])  # Adjust column names as necessary\n",
    "df_no_irrelevant_values = df_no_irrelevant_values.astype({'2': 'str', '3': 'str'})\n",
    "# Apply transformations, cleaning, and check length in one step\n",
    "formatted_and_cleaned_lines = df_no_irrelevant_values.apply(lambda row: clean_text_spacy(row[1]) + \" @\" + row[0].lower(), axis=1)\n",
    "\n",
    "# Define a minimum length for text to be included\n",
    "min_length = 20  # Example minimum length\n",
    "\n",
    "# Write the cleaned, formatted, and length-checked lines to a file\n",
    "with open('./data/train/clean_train_texts/twitter_training_clean_spacy.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in formatted_and_cleaned_lines:\n",
    "        if len(line) > min_length:  # Only write lines that are longer than the minimum length\n",
    "            file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         selected_text sentiment\n",
      "0  I`d have responded, if I were going   neutral\n",
      "1                             Sooo SAD  negative\n",
      "2                          bullying me  negative\n",
      "3                       leave me alone  negative\n",
      "4                        Sons of ****,  negative\n",
      "sentiment\n",
      "neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janne\\AppData\\Local\\Temp\\ipykernel_16028\\3789760885.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  formatted_and_cleaned_lines = df.apply(lambda row: clean_text_spacy(row[0]) + \" @\" + row[1].lower(), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# train.csv cleaning\n",
    "\n",
    "df = pd.read_csv('./data/train/train.csv', encoding='ISO-8859-1')\n",
    "column_to_keep = ['selected_text','sentiment']\n",
    "df = df[column_to_keep]\n",
    "print(df.head())\n",
    "\n",
    "value_counts = df['sentiment'].value_counts()\n",
    "print(value_counts)\n",
    "df = df.dropna(subset=['selected_text','sentiment'])\n",
    "df = df.astype({'selected_text':'str','sentiment':'str'})\n",
    "formatted_and_cleaned_lines = df.apply(lambda row: clean_text_spacy(row[0]) + \" @\" + row[1].lower(), axis=1)\n",
    "\n",
    "with open('./data/train/clean_train_texts/twitter_training_2_clean_spacy.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in formatted_and_cleaned_lines:\n",
    "        if len(line) > 10:  # Only write lines that are longer than the minimum length\n",
    "            file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  airline_sentiment                                               text\n",
      "0           neutral                @VirginAmerica What @dhepburn said.\n",
      "1          positive  @VirginAmerica plus you've added commercials t...\n",
      "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
      "3          negative  @VirginAmerica it's really aggressive to blast...\n",
      "4          negative  @VirginAmerica and it's a really big bad thing...\n",
      "  airline_sentiment                                               text\n",
      "0           neutral                                What dhepburn said.\n",
      "1          positive   plus you've added commercials to the experien...\n",
      "2           neutral   I didn't today... Must mean I need to take an...\n",
      "3          negative   it's really aggressive to blast obnoxious \"en...\n",
      "4          negative           and it's a really big bad thing about it\n",
      "airline_sentiment\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janne\\AppData\\Local\\Temp\\ipykernel_16028\\3048622738.py:21: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  formatted_and_cleaned_lines = df.apply(lambda row: clean_text_spacy(row[1]) + \" @\" + row[0].lower(), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# airline-sentiment-2-w-AA.csv cleaning\n",
    "\n",
    "df = pd.read_csv('./data/train/Airline-sentiment-2-w-AA.csv', encoding='ISO-8859-1')\n",
    "columns_to_keep = ['airline_sentiment','text']\n",
    "df = df[columns_to_keep]\n",
    "print(df.head())\n",
    "\n",
    "df['text'] = df['text'].str.replace('@','', regex=False) \\\n",
    "                        .str.replace('VirginAmerica','', regex=False) \\\n",
    "                        .str.replace('united','', regex=False) \\\n",
    "                        .str.replace('SouthwestAir','',regex=False) \\\n",
    "                        .str.replace('JetBlue','', regex=False) \\\n",
    "                        .str.replace('USAirways','',regex=False) \\\n",
    "                        .str.replace('AmericanAir','',regex=False)\n",
    "print(df.head())\n",
    "\n",
    "value_counts = df['airline_sentiment'].value_counts()\n",
    "print(value_counts)\n",
    "df = df.dropna(subset=['airline_sentiment','text'])\n",
    "df = df.astype({'airline_sentiment':'str','text':'str'})\n",
    "formatted_and_cleaned_lines = df.apply(lambda row: clean_text_spacy(row[1]) + \" @\" + row[0].lower(), axis=1)\n",
    "\n",
    "with open('./data/train/clean_train_texts/airline-sentiment-clean-spacy.txt','w', encoding='utf-8') as file:\n",
    "    for line in formatted_and_cleaned_lines:\n",
    "        if len(line) > 10:  # Only write lines that are longer than the minimum length\n",
    "            file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of our own is live Catch him here Say Streamer Shouts in chat for a chance to be in a coming Shout Out\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_line(line):\n",
    "    doc = nlp(line)\n",
    "    filtered_tokens = [token.text for token in doc if token.is_alpha and not token.text.startswith('@')]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "input_file_path = './data/train/twitter_training.csv'\n",
    "output_file_path = './data/train/clean_train_texts/twitter_training_clean_spacy.txt'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file, \\\n",
    "    open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "    for line in input_file:\n",
    "        cleaned_line = clean_line(line)\n",
    "        output_file.write(clean_line +)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
